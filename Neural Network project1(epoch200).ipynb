{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQfp7hIf8UjE"
      },
      "source": [
        "# Install Environment\n",
        "\n",
        "*   For ease of use, we recommend using Google Colab\n",
        "*   You can also use Jupyter Notebook locally (Desktop,laptop,Server)\n",
        "*   You can check this link for further [reference](https://towardsdatascience.com/how-to-set-up-anaconda-and-jupyter-notebook-the-right-way-de3b7623ea4a)\n",
        "* We recommend to install [Anaconda](https://docs.anaconda.com/anaconda/install/index.html) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html) first\n",
        "* Lastly prepare GPU for faster processing\n",
        "\n",
        "Most of the new research papers use Python3 as their programming language. \n",
        "However, if you use Colab as your training environment, Google will build that for you.\n",
        "Also, you can activate GPU by changing the Runtime in Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kak0E7knFS8a"
      },
      "source": [
        "# Install libraries\n",
        "## First install Pytorch\n",
        "Nowadays, there are many kinds of training framework in the world. Some of which support by tech companies or research centers.\n",
        "The most common are [PyTorch](https://pytorch.org/) and [Tensorflow](https://www.tensorflow.org).\n",
        "\n",
        "Here, we will use PyTorch as our framework to build our training scripts. Because it is easy-to-use and being popular for researchers, most of the papers using pytorch as their first selection.\n",
        "But if in industrial enviroment, TensorFlow would be the better selection for your product.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpBsUKCaNHHU"
      },
      "source": [
        "We will uninstall Tensorflow first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwTm1dl672f4",
        "outputId": "cda5bb38-f5a8-4f79-93e1-f6db5461846e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Successfully uninstalled tensorflow-2.11.0\n"
          ]
        }
      ],
      "source": [
        "# We won't need TensorFlow here\n",
        "!pip uninstall -y tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t05UKUpbNQ9A",
        "outputId": "48192f43-5569-41fc-b5ff-44959e1aba0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "# install PyTorch\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Gm1E67cj5Q"
      },
      "source": [
        "# Utility Functions\n",
        "Basic utility functions like saving the model to reading the data.\n",
        "As you can see we use **pickling** to save and load the checkpoints.\n",
        "You can read more about this here: https://snyk.io/blog/guide-to-python-pickle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k6HC_VEzcqu7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def pickleStore(savethings, filename):\n",
        "  # Store the checkpoint\n",
        "    dbfile = open( filename , 'wb' )\n",
        "    pickle.dump( savethings , dbfile )\n",
        "    dbfile.close()\n",
        "    return\n",
        "\n",
        "\n",
        "def pikleOpen(filename):\n",
        "  # Open the model\n",
        "    file_to_read = open( filename , \"rb\" )\n",
        "    p = pickle.load( file_to_read )\n",
        "    return p\n",
        "\n",
        "\n",
        "def readData(f):\n",
        "  # Read Data from the system\n",
        "    return np.genfromtxt(f, delimiter=',', dtype=str)[1:]\n",
        "\n",
        "\n",
        "def saveModel(net, path):\n",
        "  # Save the model\n",
        "    torch.save(net.state_dict(), path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF9CFW-ZQoJg"
      },
      "source": [
        "# Training Framework\n",
        "In order to start training, we have to follow these steps:\n",
        "\n",
        "1.  Dataset\n",
        "2.  Pre-processing\n",
        "3.  Dataloader\n",
        "4.  Model\n",
        "5.  Training Part and Testing Part\n",
        "6.  Optimizer\n",
        "7.  Loss Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MQKbSPrO9oL"
      },
      "source": [
        "## Dataset\n",
        "We need to format our data to meet the needs from models. The advantage is we load the data by batch instead of loading all of the dataset into the memory. Here we use the pre-build dataset component.\n",
        "You also can build your own Dataset object by yourself, and it should follow these methods:\n",
        "* Declaring it as an extended Dataset Object\n",
        "* Defining ``__init__``, ``__len__``, ``__getitem__`` functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xBSFkTivO75j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, data, labels, device='gpu'):\n",
        "        'Initialization'\n",
        "        self.data = data.to(device)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Load data and get label\n",
        "        X = self.data[index]\n",
        "        y = self.labels[index]\n",
        "        return X, y, index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuw1SAdZTGbn"
      },
      "source": [
        "# Pre-processing\n",
        "We need to pre-process the data before training. Preprocessing helps us to segment more data and clear out anomalies(異常) manually if present.\n",
        "It also helps us in understanding the data and thus it will eventually help in better performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U8KJx8OPTrla"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def preprocess(data, flip=True):\n",
        "    date   = data[:, 0]\n",
        "    open   = data[:, 1]\n",
        "    high   = data[:, 2]\n",
        "    low    = data[:, 3]\n",
        "    close  = data[:, 4]\n",
        "    volume = data[:, 5]\n",
        "    #array([105., 112., 110., 118., 120.])\n",
        "    prices = np.array([close for date, open, high, low, close, volume in data]).astype(np.float64)\n",
        "    #如果flip為真就使用NumPy的flip()函数将prices数组反转。这个操作可以将数据的时间顺序倒转，用于一些需要倒叙处理数据的模型。\n",
        "    if flip:\n",
        "        prices = np.flip(prices)\n",
        "    #print(prices)\n",
        "    return prices\n",
        "\n",
        "\n",
        "def train_test_split(data, percentage=0.8):\n",
        "    train_size  = int(len(data) * percentage)\n",
        "    train, test = data[:train_size], data[train_size:]\n",
        "    return train, test\n",
        "\n",
        "# 改\n",
        "def transform_dataset(dataset, look_back=20):\n",
        "    # N days as training sample\n",
        "    dataX = [dataset[i:(i + look_back)]\n",
        "            for i in range(len(dataset)-look_back-1)]\n",
        "    # 1 day as groundtruth\n",
        "    dataY = [dataset[i + look_back]\n",
        "            for i in range(len(dataset)-look_back-1)]\n",
        "    return torch.tensor(np.array(dataX), dtype=torch.float32), torch.tensor(np.array(dataY), dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seNtUE_pRboS"
      },
      "source": [
        "# DataLoader\n",
        "DataLoader provides data to trainer each batch. It fetches the data and indices from dataSet object and combine them as the output.More about parameters:\n",
        "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html.\n",
        "Fortunately, PyTorch has built in DataLoader function. But if you want to learn more, hit that link above.\n",
        "One more thing,DO NOT use test data in training.Train data and test data should be mutual exclusive.**BE CAREFUL ABOUT THIS.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bCNCwYtPUhMA"
      },
      "outputs": [],
      "source": [
        "# Demo Code\n",
        "#from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM02SQB5VcYo"
      },
      "source": [
        "# Model\n",
        "Model is the algorithm of how to map input to label. And it has three methods we need to define: ``__init__``, ``predict`` and ``forward``. \n",
        "\n",
        "**DO NOT** declare your forward layer inside forward function, cause in each epoch we need to update the parameters of each layers inside the model.\n",
        "If training with GPU, **DO NOT** forget to assign the GPU device for your model.\n",
        "You can see that instead of RNN I have used LSTM.- You can try different methods and experiment.\n",
        "\n",
        "For more details:\n",
        "* https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573\n",
        "* https://ai.stackexchange.com/questions/18198/what-is-the-difference-between-lstm-and-rnn\n",
        "* https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "* https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zc2NrxD1V6Bo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class LSTMPredictor(nn.Module):\n",
        "#改\n",
        "    def __init__(self, look_back, num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        #首先调用父类 nn.Module 的构造函数，初始化模型的基本属性。\n",
        "        super(LSTMPredictor, self).__init__()\n",
        "\n",
        "        # Nerual Layers LSTM - Long Short Term Memory\n",
        "        #定义 LSTM 层，输入维度为 look_back，输出维度为 32。这里 num_layers=2 表示 LSTM 层有两层，dropout=0.3 表示在训练时对 LSTM 输出进行 dropout，bidirectional=True 表示使用双向 LSTM。\n",
        "        self.rnn   = nn.LSTM(look_back, 32, num_layers, dropout=dropout, bidirectional=True)\n",
        "        #定义一个全连接层，输入维度为 32*(2 if bidirectional else 1)，输出维度为 16。这里 2 if bidirectional else 1 的意思是如果使用双向 LSTM，输入维度需要乘以 2。\n",
        "        self.ly_a  = nn.Linear(32*(2 if bidirectional else 1), 16)\n",
        "        # self.ly_a  = nn.Linear(look_back, 16)\n",
        "        #relu 激活函數\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.reg   = nn.Linear(16, 1)\n",
        "\n",
        "    def predict(self, input):\n",
        "        with torch.no_grad():\n",
        "            return self.forward(input).item()\n",
        "    #forward 函數是nn.Module類中的一個方法\n",
        "    def forward(self, input):\n",
        "        r_out, (h_n, h_c) = self.rnn(input.unsqueeze(1), None)\n",
        "        # print(r_out.shape)\n",
        "        # input()\n",
        "        logits = self.reg(self.relu(self.ly_a(r_out.squeeze(1))))\n",
        "        # logits = self.reg(self.relu(self.ly_a(input)))\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyTEMBRgbRNA"
      },
      "source": [
        "# Training and Testing part\n",
        "We finally define the Dataset, DataLoader and Model. But there are still some more things that need to be done. We need to input the data to model, get the predicted output, calculate the criterion and update the parameters of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IBfVqwgdeIt"
      },
      "source": [
        "## Training Function\n",
        "First is the training part. Training map the relationship between input and label, also update the parameters at the stage.\n",
        "**DO NOT** forget to move your data to **GPU**, model and data should both at the same place.\n",
        "One more thing, we suggest writing the checkpoint if your experiment taking too long time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x2aZelKYcL-_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "# insert at 1, 0 is the script path (or '' in REPL)\n",
        "sys.path.insert(1, '../')\n",
        "\n",
        "\n",
        "\n",
        "def trainer(net, criterion, optimizer, trainloader, devloader, epoch_n=100, path=\"./checkpoint/save.pt\"):\n",
        "    train_losses, valid_losses = [], []\n",
        "    for epoch in range(epoch_n): # loop over the dataset multiple times\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        #对于每个训练样本，使用enumerate()函数获得其索引和数据。\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the iputs; data is a list of [inputs, labels]\n",
        "            inputs, labels, data_index = data\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs.cpu(), labels.unsqueeze(1).cpu())\n",
        "            train_loss += loss.item()*inputs.shape[0]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 2000 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 2000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        net.eval()\n",
        "        for i, data in enumerate(devloader, 0):\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            inputs, labels, data_index = data\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            outputs = net(inputs)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(outputs.cpu(), labels.cpu())\n",
        "            # update average validation loss \n",
        "            valid_loss += loss.item()*inputs.shape[0]\n",
        "        \n",
        "        # calculate average losses\n",
        "        train_loss = train_loss/len(trainloader.dataset)\n",
        "        valid_loss = valid_loss/len(devloader.dataset)\n",
        "    \n",
        "        # print training/validation statistics \n",
        "        print('\\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(train_loss, valid_loss))\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "    print('Finished Training')\n",
        "    plt.plot(range(1, epoch_n+1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, epoch_n+1), valid_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    ## Save model\n",
        "    saveModel(net, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jqonn-Ueddc"
      },
      "source": [
        "Here,\n",
        "1. ``optimizer.zero_grad`` means to set the gradient to zero. In each batch, the loss value isaccumulated from the samples' loss value.\n",
        "2.``criterion`` is a pre-defined function. It is used for calculating the loss value.\n",
        "3.``loss.backward`` here means to do back propagation based on the loss value and calculate thegradient value.\n",
        "4.``optimizer.step`` means to do gradient descent. \n",
        "\n",
        "More information about gradient descent https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zqqs5k8dRnO"
      },
      "source": [
        "## Test Function\n",
        "Next, we define the testing function. Testing just show how the performance of your model is, it does not upgrade any parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WuHKHR9oeB_V"
      },
      "outputs": [],
      "source": [
        "def tester(net, criterion, testloader):\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels, data_index = data\n",
        "            outputs = net(inputs)\n",
        "            loss += criterion(outputs.cpu(), labels.unsqueeze(1).cpu())\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umfXtyuD5qk3"
      },
      "source": [
        "# Optimizer\n",
        "Optimizer is the way for models to do gradient discent. It helps model to adjust its parameters. For starters in machine learning, we suggest using\n",
        "**Adam** as your Optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tpFg-Air6zOm"
      },
      "outputs": [],
      "source": [
        "# Demo Code\n",
        "# lr =0.001\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yWMgK0P6JVv"
      },
      "source": [
        "# Loss Function\n",
        "It can be also called as **loss function** or **cost function**. More specific detail please visit:\n",
        "* https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing\n",
        "* https://medium.com/@gatorsquare/ml-gradient-descent-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-c664b5874e5c\n",
        "A loss function is a function that maps an event or values of one or more variables onto a realnumber intuitively representing some \"cost\" associated with the event, which measures thepenalty. And an optimization problem seeks to minimize a loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lS74VDuA6ubv"
      },
      "outputs": [],
      "source": [
        "# Demo Code\n",
        "#criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8VLpMF37BY0"
      },
      "source": [
        "# All set!\n",
        "A few more steps and then we can start training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "565GfonCNNTk"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tFuSdQsROJDJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "import os\n",
        "import math\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSmWHpZ_72f9"
      },
      "source": [
        "## Get Device for Training\n",
        "We want to be able to train our model on a hardware accelerator like the GPU,\n",
        "if it is available. Let's check to see if\n",
        "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) is available, else we\n",
        "continue to use the GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "704KLUwr72f-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456d427c-785e-4df6-ad2d-c2223eb342f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FngTb1pG7WKt"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9OwoYNQw7Vvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c117bb-dee7-40cc-a782-56d30f6097a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of samples: 3724\n"
          ]
        }
      ],
      "source": [
        "data = readData(\"0050_history.csv\")\n",
        "print('Num of samples:', len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVxzAUts9slb"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5VlHYPNX8Q6x"
      },
      "outputs": [],
      "source": [
        "prices = preprocess(data)\n",
        "# Divide trainset and test set\n",
        "train, test = train_test_split(prices, 0.8)\n",
        "# Set the N(look_back)=5 because from the five day stock, we are predicting the next day\n",
        "look_back = 5\n",
        "trainX, trainY = transform_dataset(train, look_back)\n",
        "testX, testY   = transform_dataset(test, look_back)\n",
        "# Get dataset\n",
        "trainset = Dataset(trainX, trainY, device)\n",
        "testset  = Dataset(testX, testY, device)\n",
        "# Get dataloader\n",
        "#改\n",
        "batch_size = 100\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0) # num_workers should set 1 if put data on CUDA\n",
        "testloader  = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ungZ7Pzt96JG"
      },
      "source": [
        "# Model Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LvxT4_Uz90Bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc409eaf-3706-47f6-a739-113963bcacd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMPredictor(\n",
              "  (rnn): LSTM(5, 32, num_layers=2, dropout=0.3, bidirectional=True)\n",
              "  (ly_a): Linear(in_features=64, out_features=16, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (reg): Linear(in_features=16, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "net = LSTMPredictor(look_back)\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmtWM6li-AQT"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "67dCbOxz99zx"
      },
      "outputs": [],
      "source": [
        "#改\n",
        "criterion = nn.MSELoss() # Feel free to use any other loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aol9u-Kn-WJy"
      },
      "source": [
        "I used Mean Square Error Loss function. You can tweak with other if you want.\n",
        "https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\n",
        "Or, you can create your own loss function \n",
        "https://towardsdatascience.com/how-to-create-a-custom-loss-function-keras-3a89156ec69b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbKMukjw-14m"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Gj12NGdq-FEy"
      },
      "outputs": [],
      "source": [
        "#改\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001) # you can tweak the lr and see if it affects anything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R92TxhjM_DcD"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgOWQ62P_070"
      },
      "source": [
        "You can change the epoch here.\n",
        "But also make sure it doesn't overfit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pMlDX-Ru_Czv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "efeeb725-5595-4dbf-abbb-7d338cd319b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    10] loss: 36.197\n",
            "[1,    20] loss: 35.675\n",
            "[1,    30] loss: 34.540\n",
            "\tTraining Loss: 7091.681717 \tValidation Loss: 2465.476244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([39])) that is different to the input size (torch.Size([39, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2,    10] loss: 34.108\n",
            "[2,    20] loss: 32.773\n",
            "[2,    30] loss: 31.855\n",
            "\tTraining Loss: 6579.711428 \tValidation Loss: 2011.268339\n",
            "[3,    10] loss: 30.488\n",
            "[3,    20] loss: 28.562\n",
            "[3,    30] loss: 27.286\n",
            "\tTraining Loss: 5761.061484 \tValidation Loss: 1515.504624\n",
            "[4,    10] loss: 25.231\n",
            "[4,    20] loss: 25.268\n",
            "[4,    30] loss: 22.333\n",
            "\tTraining Loss: 4860.520874 \tValidation Loss: 1017.003186\n",
            "[5,    10] loss: 21.151\n",
            "[5,    20] loss: 19.535\n",
            "[5,    30] loss: 18.090\n",
            "\tTraining Loss: 3923.415346 \tValidation Loss: 580.129316\n",
            "[6,    10] loss: 16.527\n",
            "[6,    20] loss: 14.756\n",
            "[6,    30] loss: 14.151\n",
            "\tTraining Loss: 3026.338285 \tValidation Loss: 262.673952\n",
            "[7,    10] loss: 11.640\n",
            "[7,    20] loss: 12.272\n",
            "[7,    30] loss: 9.765\n",
            "\tTraining Loss: 2246.517935 \tValidation Loss: 99.361596\n",
            "[8,    10] loss: 8.826\n",
            "[8,    20] loss: 7.525\n",
            "[8,    30] loss: 8.099\n",
            "\tTraining Loss: 1633.044092 \tValidation Loss: 92.666313\n",
            "[9,    10] loss: 6.449\n",
            "[9,    20] loss: 6.232\n",
            "[9,    30] loss: 5.403\n",
            "\tTraining Loss: 1206.044470 \tValidation Loss: 209.680944\n",
            "[10,    10] loss: 5.007\n",
            "[10,    20] loss: 4.623\n",
            "[10,    30] loss: 4.520\n",
            "\tTraining Loss: 942.873104 \tValidation Loss: 388.280865\n",
            "[11,    10] loss: 4.119\n",
            "[11,    20] loss: 4.083\n",
            "[11,    30] loss: 3.856\n",
            "\tTraining Loss: 804.778083 \tValidation Loss: 574.453524\n",
            "[12,    10] loss: 3.586\n",
            "[12,    20] loss: 3.921\n",
            "[12,    30] loss: 3.558\n",
            "\tTraining Loss: 737.469562 \tValidation Loss: 727.245559\n",
            "[13,    10] loss: 3.907\n",
            "[13,    20] loss: 3.439\n",
            "[13,    30] loss: 3.401\n",
            "\tTraining Loss: 714.392078 \tValidation Loss: 838.534094\n",
            "[14,    10] loss: 3.428\n",
            "[14,    20] loss: 3.496\n",
            "[14,    30] loss: 3.650\n",
            "\tTraining Loss: 706.010419 \tValidation Loss: 897.531340\n",
            "[15,    10] loss: 3.384\n",
            "[15,    20] loss: 3.501\n",
            "[15,    30] loss: 3.706\n",
            "\tTraining Loss: 705.673987 \tValidation Loss: 931.426132\n",
            "[16,    10] loss: 3.652\n",
            "[16,    20] loss: 3.466\n",
            "[16,    30] loss: 3.471\n",
            "\tTraining Loss: 704.517414 \tValidation Loss: 959.216682\n",
            "[17,    10] loss: 3.684\n",
            "[17,    20] loss: 3.426\n",
            "[17,    30] loss: 3.464\n",
            "\tTraining Loss: 704.935878 \tValidation Loss: 966.233911\n",
            "[18,    10] loss: 3.670\n",
            "[18,    20] loss: 3.433\n",
            "[18,    30] loss: 3.487\n",
            "\tTraining Loss: 705.401227 \tValidation Loss: 965.179535\n",
            "[19,    10] loss: 3.537\n",
            "[19,    20] loss: 3.499\n",
            "[19,    30] loss: 3.509\n",
            "\tTraining Loss: 704.565198 \tValidation Loss: 969.525901\n",
            "[20,    10] loss: 3.496\n",
            "[20,    20] loss: 3.703\n",
            "[20,    30] loss: 3.333\n",
            "\tTraining Loss: 701.778349 \tValidation Loss: 973.445904\n",
            "[21,    10] loss: 3.457\n",
            "[21,    20] loss: 3.542\n",
            "[21,    30] loss: 3.523\n",
            "\tTraining Loss: 701.664439 \tValidation Loss: 970.874362\n",
            "[22,    10] loss: 3.527\n",
            "[22,    20] loss: 3.377\n",
            "[22,    30] loss: 3.600\n",
            "\tTraining Loss: 700.481804 \tValidation Loss: 968.003140\n",
            "[23,    10] loss: 3.564\n",
            "[23,    20] loss: 3.716\n",
            "[23,    30] loss: 3.266\n",
            "\tTraining Loss: 703.849190 \tValidation Loss: 973.854621\n",
            "[24,    10] loss: 3.405\n",
            "[24,    20] loss: 3.590\n",
            "[24,    30] loss: 3.561\n",
            "\tTraining Loss: 704.341869 \tValidation Loss: 968.497608\n",
            "[25,    10] loss: 3.731\n",
            "[25,    20] loss: 3.402\n",
            "[25,    30] loss: 3.412\n",
            "\tTraining Loss: 703.042986 \tValidation Loss: 966.837381\n",
            "[26,    10] loss: 3.667\n",
            "[26,    20] loss: 3.328\n",
            "[26,    30] loss: 3.443\n",
            "\tTraining Loss: 696.685297 \tValidation Loss: 849.418541\n",
            "[27,    10] loss: 3.406\n",
            "[27,    20] loss: 3.271\n",
            "[27,    30] loss: 2.758\n",
            "\tTraining Loss: 629.838931 \tValidation Loss: 345.462905\n",
            "[28,    10] loss: 2.498\n",
            "[28,    20] loss: 2.320\n",
            "[28,    30] loss: 2.078\n",
            "\tTraining Loss: 459.569989 \tValidation Loss: 119.211304\n",
            "[29,    10] loss: 1.749\n",
            "[29,    20] loss: 1.658\n",
            "[29,    30] loss: 1.319\n",
            "\tTraining Loss: 314.878212 \tValidation Loss: 78.731374\n",
            "[30,    10] loss: 1.311\n",
            "[30,    20] loss: 1.120\n",
            "[30,    30] loss: 0.973\n",
            "\tTraining Loss: 227.323067 \tValidation Loss: 55.459086\n",
            "[31,    10] loss: 0.844\n",
            "[31,    20] loss: 0.869\n",
            "[31,    30] loss: 0.792\n",
            "\tTraining Loss: 166.924925 \tValidation Loss: 49.979082\n",
            "[32,    10] loss: 0.672\n",
            "[32,    20] loss: 0.586\n",
            "[32,    30] loss: 0.633\n",
            "\tTraining Loss: 126.328727 \tValidation Loss: 47.105278\n",
            "[33,    10] loss: 0.591\n",
            "[33,    20] loss: 0.471\n",
            "[33,    30] loss: 0.452\n",
            "\tTraining Loss: 100.515744 \tValidation Loss: 52.855389\n",
            "[34,    10] loss: 0.463\n",
            "[34,    20] loss: 0.385\n",
            "[34,    30] loss: 0.331\n",
            "\tTraining Loss: 78.916183 \tValidation Loss: 52.006817\n",
            "[35,    10] loss: 0.344\n",
            "[35,    20] loss: 0.328\n",
            "[35,    30] loss: 0.308\n",
            "\tTraining Loss: 65.455557 \tValidation Loss: 45.971159\n",
            "[36,    10] loss: 0.312\n",
            "[36,    20] loss: 0.287\n",
            "[36,    30] loss: 0.232\n",
            "\tTraining Loss: 55.632239 \tValidation Loss: 47.708042\n",
            "[37,    10] loss: 0.266\n",
            "[37,    20] loss: 0.214\n",
            "[37,    30] loss: 0.233\n",
            "\tTraining Loss: 47.366954 \tValidation Loss: 40.380291\n",
            "[38,    10] loss: 0.207\n",
            "[38,    20] loss: 0.178\n",
            "[38,    30] loss: 0.195\n",
            "\tTraining Loss: 38.608040 \tValidation Loss: 40.483861\n",
            "[39,    10] loss: 0.186\n",
            "[39,    20] loss: 0.190\n",
            "[39,    30] loss: 0.153\n",
            "\tTraining Loss: 35.247829 \tValidation Loss: 40.134821\n",
            "[40,    10] loss: 0.158\n",
            "[40,    20] loss: 0.152\n",
            "[40,    30] loss: 0.167\n",
            "\tTraining Loss: 31.736485 \tValidation Loss: 44.774253\n",
            "[41,    10] loss: 0.139\n",
            "[41,    20] loss: 0.135\n",
            "[41,    30] loss: 0.133\n",
            "\tTraining Loss: 27.203271 \tValidation Loss: 50.025449\n",
            "[42,    10] loss: 0.135\n",
            "[42,    20] loss: 0.116\n",
            "[42,    30] loss: 0.126\n",
            "\tTraining Loss: 25.120773 \tValidation Loss: 48.049281\n",
            "[43,    10] loss: 0.120\n",
            "[43,    20] loss: 0.126\n",
            "[43,    30] loss: 0.118\n",
            "\tTraining Loss: 24.041258 \tValidation Loss: 61.204614\n",
            "[44,    10] loss: 0.102\n",
            "[44,    20] loss: 0.102\n",
            "[44,    30] loss: 0.105\n",
            "\tTraining Loss: 20.566091 \tValidation Loss: 68.256383\n",
            "[45,    10] loss: 0.106\n",
            "[45,    20] loss: 0.102\n",
            "[45,    30] loss: 0.101\n",
            "\tTraining Loss: 20.501459 \tValidation Loss: 67.143794\n",
            "[46,    10] loss: 0.103\n",
            "[46,    20] loss: 0.096\n",
            "[46,    30] loss: 0.093\n",
            "\tTraining Loss: 19.499607 \tValidation Loss: 65.694730\n",
            "[47,    10] loss: 0.094\n",
            "[47,    20] loss: 0.084\n",
            "[47,    30] loss: 0.100\n",
            "\tTraining Loss: 18.425561 \tValidation Loss: 68.458109\n",
            "[48,    10] loss: 0.094\n",
            "[48,    20] loss: 0.076\n",
            "[48,    30] loss: 0.080\n",
            "\tTraining Loss: 16.697887 \tValidation Loss: 74.122610\n",
            "[49,    10] loss: 0.096\n",
            "[49,    20] loss: 0.087\n",
            "[49,    30] loss: 0.117\n",
            "\tTraining Loss: 19.856651 \tValidation Loss: 87.429718\n",
            "[50,    10] loss: 0.119\n",
            "[50,    20] loss: 0.107\n",
            "[50,    30] loss: 0.082\n",
            "\tTraining Loss: 20.610382 \tValidation Loss: 71.578327\n",
            "[51,    10] loss: 0.087\n",
            "[51,    20] loss: 0.078\n",
            "[51,    30] loss: 0.069\n",
            "\tTraining Loss: 15.538169 \tValidation Loss: 77.840947\n",
            "[52,    10] loss: 0.094\n",
            "[52,    20] loss: 0.079\n",
            "[52,    30] loss: 0.095\n",
            "\tTraining Loss: 17.843636 \tValidation Loss: 72.309950\n",
            "[53,    10] loss: 0.086\n",
            "[53,    20] loss: 0.097\n",
            "[53,    30] loss: 0.080\n",
            "\tTraining Loss: 17.606458 \tValidation Loss: 73.544421\n",
            "[54,    10] loss: 0.076\n",
            "[54,    20] loss: 0.070\n",
            "[54,    30] loss: 0.085\n",
            "\tTraining Loss: 15.147132 \tValidation Loss: 79.619207\n",
            "[55,    10] loss: 0.078\n",
            "[55,    20] loss: 0.064\n",
            "[55,    30] loss: 0.061\n",
            "\tTraining Loss: 13.534770 \tValidation Loss: 168.872347\n",
            "[56,    10] loss: 0.066\n",
            "[56,    20] loss: 0.081\n",
            "[56,    30] loss: 0.072\n",
            "\tTraining Loss: 14.583887 \tValidation Loss: 130.509134\n",
            "[57,    10] loss: 0.066\n",
            "[57,    20] loss: 0.076\n",
            "[57,    30] loss: 0.083\n",
            "\tTraining Loss: 15.000825 \tValidation Loss: 173.816717\n",
            "[58,    10] loss: 0.059\n",
            "[58,    20] loss: 0.088\n",
            "[58,    30] loss: 0.072\n",
            "\tTraining Loss: 14.470969 \tValidation Loss: 163.995132\n",
            "[59,    10] loss: 0.060\n",
            "[59,    20] loss: 0.061\n",
            "[59,    30] loss: 0.062\n",
            "\tTraining Loss: 12.265033 \tValidation Loss: 192.755480\n",
            "[60,    10] loss: 0.064\n",
            "[60,    20] loss: 0.063\n",
            "[60,    30] loss: 0.082\n",
            "\tTraining Loss: 13.864986 \tValidation Loss: 198.311863\n",
            "[61,    10] loss: 0.064\n",
            "[61,    20] loss: 0.059\n",
            "[61,    30] loss: 0.064\n",
            "\tTraining Loss: 12.470096 \tValidation Loss: 172.381491\n",
            "[62,    10] loss: 0.062\n",
            "[62,    20] loss: 0.061\n",
            "[62,    30] loss: 0.056\n",
            "\tTraining Loss: 11.861547 \tValidation Loss: 182.236728\n",
            "[63,    10] loss: 0.053\n",
            "[63,    20] loss: 0.064\n",
            "[63,    30] loss: 0.061\n",
            "\tTraining Loss: 11.872791 \tValidation Loss: 125.850378\n",
            "[64,    10] loss: 0.058\n",
            "[64,    20] loss: 0.067\n",
            "[64,    30] loss: 0.064\n",
            "\tTraining Loss: 12.555909 \tValidation Loss: 208.163946\n",
            "[65,    10] loss: 0.060\n",
            "[65,    20] loss: 0.072\n",
            "[65,    30] loss: 0.069\n",
            "\tTraining Loss: 13.357165 \tValidation Loss: 189.289624\n",
            "[66,    10] loss: 0.064\n",
            "[66,    20] loss: 0.070\n",
            "[66,    30] loss: 0.069\n",
            "\tTraining Loss: 13.496713 \tValidation Loss: 243.061060\n",
            "[67,    10] loss: 0.056\n",
            "[67,    20] loss: 0.059\n",
            "[67,    30] loss: 0.065\n",
            "\tTraining Loss: 12.014735 \tValidation Loss: 195.640673\n",
            "[68,    10] loss: 0.073\n",
            "[68,    20] loss: 0.071\n",
            "[68,    30] loss: 0.078\n",
            "\tTraining Loss: 14.741011 \tValidation Loss: 225.631310\n",
            "[69,    10] loss: 0.063\n",
            "[69,    20] loss: 0.057\n",
            "[69,    30] loss: 0.052\n",
            "\tTraining Loss: 11.490256 \tValidation Loss: 235.605751\n",
            "[70,    10] loss: 0.059\n",
            "[70,    20] loss: 0.057\n",
            "[70,    30] loss: 0.058\n",
            "\tTraining Loss: 11.552135 \tValidation Loss: 244.080270\n",
            "[71,    10] loss: 0.053\n",
            "[71,    20] loss: 0.057\n",
            "[71,    30] loss: 0.055\n",
            "\tTraining Loss: 11.001210 \tValidation Loss: 222.672966\n",
            "[72,    10] loss: 0.067\n",
            "[72,    20] loss: 0.061\n",
            "[72,    30] loss: 0.054\n",
            "\tTraining Loss: 12.229953 \tValidation Loss: 241.394018\n",
            "[73,    10] loss: 0.060\n",
            "[73,    20] loss: 0.049\n",
            "[73,    30] loss: 0.058\n",
            "\tTraining Loss: 11.099768 \tValidation Loss: 234.929234\n",
            "[74,    10] loss: 0.055\n",
            "[74,    20] loss: 0.064\n",
            "[74,    30] loss: 0.057\n",
            "\tTraining Loss: 11.739245 \tValidation Loss: 244.832834\n",
            "[75,    10] loss: 0.065\n",
            "[75,    20] loss: 0.057\n",
            "[75,    30] loss: 0.058\n",
            "\tTraining Loss: 11.991416 \tValidation Loss: 253.364309\n",
            "[76,    10] loss: 0.054\n",
            "[76,    20] loss: 0.052\n",
            "[76,    30] loss: 0.061\n",
            "\tTraining Loss: 11.199041 \tValidation Loss: 228.464681\n",
            "[77,    10] loss: 0.063\n",
            "[77,    20] loss: 0.048\n",
            "[77,    30] loss: 0.057\n",
            "\tTraining Loss: 11.167112 \tValidation Loss: 302.303162\n",
            "[78,    10] loss: 0.058\n",
            "[78,    20] loss: 0.074\n",
            "[78,    30] loss: 0.054\n",
            "\tTraining Loss: 12.384900 \tValidation Loss: 285.781626\n",
            "[79,    10] loss: 0.056\n",
            "[79,    20] loss: 0.050\n",
            "[79,    30] loss: 0.052\n",
            "\tTraining Loss: 10.548964 \tValidation Loss: 289.452803\n",
            "[80,    10] loss: 0.059\n",
            "[80,    20] loss: 0.050\n",
            "[80,    30] loss: 0.050\n",
            "\tTraining Loss: 10.635757 \tValidation Loss: 327.176339\n",
            "[81,    10] loss: 0.056\n",
            "[81,    20] loss: 0.067\n",
            "[81,    30] loss: 0.056\n",
            "\tTraining Loss: 11.889173 \tValidation Loss: 262.840299\n",
            "[82,    10] loss: 0.066\n",
            "[82,    20] loss: 0.060\n",
            "[82,    30] loss: 0.062\n",
            "\tTraining Loss: 12.542086 \tValidation Loss: 306.683318\n",
            "[83,    10] loss: 0.068\n",
            "[83,    20] loss: 0.069\n",
            "[83,    30] loss: 0.059\n",
            "\tTraining Loss: 13.008595 \tValidation Loss: 260.776841\n",
            "[84,    10] loss: 0.064\n",
            "[84,    20] loss: 0.063\n",
            "[84,    30] loss: 0.062\n",
            "\tTraining Loss: 12.469723 \tValidation Loss: 262.694164\n",
            "[85,    10] loss: 0.055\n",
            "[85,    20] loss: 0.057\n",
            "[85,    30] loss: 0.054\n",
            "\tTraining Loss: 11.122163 \tValidation Loss: 303.630418\n",
            "[86,    10] loss: 0.050\n",
            "[86,    20] loss: 0.051\n",
            "[86,    30] loss: 0.054\n",
            "\tTraining Loss: 10.292635 \tValidation Loss: 290.102213\n",
            "[87,    10] loss: 0.051\n",
            "[87,    20] loss: 0.046\n",
            "[87,    30] loss: 0.045\n",
            "\tTraining Loss: 9.473603 \tValidation Loss: 308.954959\n",
            "[88,    10] loss: 0.055\n",
            "[88,    20] loss: 0.046\n",
            "[88,    30] loss: 0.049\n",
            "\tTraining Loss: 10.016037 \tValidation Loss: 286.745052\n",
            "[89,    10] loss: 0.052\n",
            "[89,    20] loss: 0.047\n",
            "[89,    30] loss: 0.054\n",
            "\tTraining Loss: 10.271674 \tValidation Loss: 286.660386\n",
            "[90,    10] loss: 0.050\n",
            "[90,    20] loss: 0.048\n",
            "[90,    30] loss: 0.054\n",
            "\tTraining Loss: 10.113772 \tValidation Loss: 283.024793\n",
            "[91,    10] loss: 0.059\n",
            "[91,    20] loss: 0.058\n",
            "[91,    30] loss: 0.060\n",
            "\tTraining Loss: 11.753639 \tValidation Loss: 287.949112\n",
            "[92,    10] loss: 0.057\n",
            "[92,    20] loss: 0.054\n",
            "[92,    30] loss: 0.049\n",
            "\tTraining Loss: 10.683749 \tValidation Loss: 286.345985\n",
            "[93,    10] loss: 0.046\n",
            "[93,    20] loss: 0.045\n",
            "[93,    30] loss: 0.053\n",
            "\tTraining Loss: 9.529503 \tValidation Loss: 292.314625\n",
            "[94,    10] loss: 0.047\n",
            "[94,    20] loss: 0.048\n",
            "[94,    30] loss: 0.048\n",
            "\tTraining Loss: 9.556583 \tValidation Loss: 296.882976\n",
            "[95,    10] loss: 0.048\n",
            "[95,    20] loss: 0.051\n",
            "[95,    30] loss: 0.055\n",
            "\tTraining Loss: 10.231658 \tValidation Loss: 310.639625\n",
            "[96,    10] loss: 0.054\n",
            "[96,    20] loss: 0.046\n",
            "[96,    30] loss: 0.043\n",
            "\tTraining Loss: 9.549150 \tValidation Loss: 286.922153\n",
            "[97,    10] loss: 0.048\n",
            "[97,    20] loss: 0.055\n",
            "[97,    30] loss: 0.053\n",
            "\tTraining Loss: 10.399671 \tValidation Loss: 302.595925\n",
            "[98,    10] loss: 0.050\n",
            "[98,    20] loss: 0.048\n",
            "[98,    30] loss: 0.046\n",
            "\tTraining Loss: 9.598581 \tValidation Loss: 307.119613\n",
            "[99,    10] loss: 0.070\n",
            "[99,    20] loss: 0.046\n",
            "[99,    30] loss: 0.043\n",
            "\tTraining Loss: 10.601261 \tValidation Loss: 295.326529\n",
            "[100,    10] loss: 0.046\n",
            "[100,    20] loss: 0.049\n",
            "[100,    30] loss: 0.056\n",
            "\tTraining Loss: 9.988837 \tValidation Loss: 305.045842\n",
            "[101,    10] loss: 0.043\n",
            "[101,    20] loss: 0.046\n",
            "[101,    30] loss: 0.051\n",
            "\tTraining Loss: 9.303789 \tValidation Loss: 300.333518\n",
            "[102,    10] loss: 0.042\n",
            "[102,    20] loss: 0.054\n",
            "[102,    30] loss: 0.050\n",
            "\tTraining Loss: 9.743937 \tValidation Loss: 314.761404\n",
            "[103,    10] loss: 0.047\n",
            "[103,    20] loss: 0.044\n",
            "[103,    30] loss: 0.052\n",
            "\tTraining Loss: 9.519282 \tValidation Loss: 313.343136\n",
            "[104,    10] loss: 0.047\n",
            "[104,    20] loss: 0.047\n",
            "[104,    30] loss: 0.051\n",
            "\tTraining Loss: 9.614616 \tValidation Loss: 310.120093\n",
            "[105,    10] loss: 0.049\n",
            "[105,    20] loss: 0.046\n",
            "[105,    30] loss: 0.053\n",
            "\tTraining Loss: 9.772210 \tValidation Loss: 309.375402\n",
            "[106,    10] loss: 0.044\n",
            "[106,    20] loss: 0.046\n",
            "[106,    30] loss: 0.042\n",
            "\tTraining Loss: 8.825347 \tValidation Loss: 330.805757\n",
            "[107,    10] loss: 0.039\n",
            "[107,    20] loss: 0.051\n",
            "[107,    30] loss: 0.051\n",
            "\tTraining Loss: 9.452126 \tValidation Loss: 303.707423\n",
            "[108,    10] loss: 0.047\n",
            "[108,    20] loss: 0.043\n",
            "[108,    30] loss: 0.052\n",
            "\tTraining Loss: 9.412158 \tValidation Loss: 325.047964\n",
            "[109,    10] loss: 0.053\n",
            "[109,    20] loss: 0.045\n",
            "[109,    30] loss: 0.057\n",
            "\tTraining Loss: 10.226080 \tValidation Loss: 313.903327\n",
            "[110,    10] loss: 0.050\n",
            "[110,    20] loss: 0.048\n",
            "[110,    30] loss: 0.051\n",
            "\tTraining Loss: 9.976407 \tValidation Loss: 317.038292\n",
            "[111,    10] loss: 0.046\n",
            "[111,    20] loss: 0.059\n",
            "[111,    30] loss: 0.048\n",
            "\tTraining Loss: 10.128391 \tValidation Loss: 319.548128\n",
            "[112,    10] loss: 0.043\n",
            "[112,    20] loss: 0.040\n",
            "[112,    30] loss: 0.048\n",
            "\tTraining Loss: 8.712220 \tValidation Loss: 323.735799\n",
            "[113,    10] loss: 0.046\n",
            "[113,    20] loss: 0.054\n",
            "[113,    30] loss: 0.047\n",
            "\tTraining Loss: 9.817593 \tValidation Loss: 313.960164\n",
            "[114,    10] loss: 0.050\n",
            "[114,    20] loss: 0.044\n",
            "[114,    30] loss: 0.044\n",
            "\tTraining Loss: 9.226036 \tValidation Loss: 307.906450\n",
            "[115,    10] loss: 0.045\n",
            "[115,    20] loss: 0.047\n",
            "[115,    30] loss: 0.049\n",
            "\tTraining Loss: 9.413381 \tValidation Loss: 319.005963\n",
            "[116,    10] loss: 0.043\n",
            "[116,    20] loss: 0.044\n",
            "[116,    30] loss: 0.043\n",
            "\tTraining Loss: 8.666996 \tValidation Loss: 332.134976\n",
            "[117,    10] loss: 0.045\n",
            "[117,    20] loss: 0.044\n",
            "[117,    30] loss: 0.044\n",
            "\tTraining Loss: 8.905338 \tValidation Loss: 334.936141\n",
            "[118,    10] loss: 0.058\n",
            "[118,    20] loss: 0.045\n",
            "[118,    30] loss: 0.046\n",
            "\tTraining Loss: 9.893210 \tValidation Loss: 336.768789\n",
            "[119,    10] loss: 0.047\n",
            "[119,    20] loss: 0.052\n",
            "[119,    30] loss: 0.054\n",
            "\tTraining Loss: 10.198651 \tValidation Loss: 318.525713\n",
            "[120,    10] loss: 0.044\n",
            "[120,    20] loss: 0.040\n",
            "[120,    30] loss: 0.048\n",
            "\tTraining Loss: 8.762601 \tValidation Loss: 333.661722\n",
            "[121,    10] loss: 0.045\n",
            "[121,    20] loss: 0.048\n",
            "[121,    30] loss: 0.055\n",
            "\tTraining Loss: 9.786577 \tValidation Loss: 329.251275\n",
            "[122,    10] loss: 0.046\n",
            "[122,    20] loss: 0.040\n",
            "[122,    30] loss: 0.050\n",
            "\tTraining Loss: 9.085396 \tValidation Loss: 327.659032\n",
            "[123,    10] loss: 0.044\n",
            "[123,    20] loss: 0.045\n",
            "[123,    30] loss: 0.040\n",
            "\tTraining Loss: 8.566090 \tValidation Loss: 318.332543\n",
            "[124,    10] loss: 0.043\n",
            "[124,    20] loss: 0.057\n",
            "[124,    30] loss: 0.042\n",
            "\tTraining Loss: 9.448168 \tValidation Loss: 325.475743\n",
            "[125,    10] loss: 0.047\n",
            "[125,    20] loss: 0.050\n",
            "[125,    30] loss: 0.036\n",
            "\tTraining Loss: 8.881321 \tValidation Loss: 343.793581\n",
            "[126,    10] loss: 0.036\n",
            "[126,    20] loss: 0.048\n",
            "[126,    30] loss: 0.041\n",
            "\tTraining Loss: 8.319912 \tValidation Loss: 426.151733\n",
            "[127,    10] loss: 0.035\n",
            "[127,    20] loss: 0.044\n",
            "[127,    30] loss: 0.047\n",
            "\tTraining Loss: 8.399807 \tValidation Loss: 401.498442\n",
            "[128,    10] loss: 0.049\n",
            "[128,    20] loss: 0.042\n",
            "[128,    30] loss: 0.052\n",
            "\tTraining Loss: 9.500562 \tValidation Loss: 466.265892\n",
            "[129,    10] loss: 0.039\n",
            "[129,    20] loss: 0.043\n",
            "[129,    30] loss: 0.040\n",
            "\tTraining Loss: 8.083281 \tValidation Loss: 434.968751\n",
            "[130,    10] loss: 0.036\n",
            "[130,    20] loss: 0.035\n",
            "[130,    30] loss: 0.045\n",
            "\tTraining Loss: 7.733307 \tValidation Loss: 433.826136\n",
            "[131,    10] loss: 0.042\n",
            "[131,    20] loss: 0.043\n",
            "[131,    30] loss: 0.046\n",
            "\tTraining Loss: 8.730470 \tValidation Loss: 347.823041\n",
            "[132,    10] loss: 0.037\n",
            "[132,    20] loss: 0.040\n",
            "[132,    30] loss: 0.042\n",
            "\tTraining Loss: 7.866009 \tValidation Loss: 332.923242\n",
            "[133,    10] loss: 0.042\n",
            "[133,    20] loss: 0.039\n",
            "[133,    30] loss: 0.042\n",
            "\tTraining Loss: 8.157435 \tValidation Loss: 360.614700\n",
            "[134,    10] loss: 0.037\n",
            "[134,    20] loss: 0.040\n",
            "[134,    30] loss: 0.044\n",
            "\tTraining Loss: 8.118457 \tValidation Loss: 335.740452\n",
            "[135,    10] loss: 0.043\n",
            "[135,    20] loss: 0.040\n",
            "[135,    30] loss: 0.044\n",
            "\tTraining Loss: 8.432060 \tValidation Loss: 341.520532\n",
            "[136,    10] loss: 0.043\n",
            "[136,    20] loss: 0.043\n",
            "[136,    30] loss: 0.038\n",
            "\tTraining Loss: 8.253663 \tValidation Loss: 320.102152\n",
            "[137,    10] loss: 0.040\n",
            "[137,    20] loss: 0.043\n",
            "[137,    30] loss: 0.038\n",
            "\tTraining Loss: 8.118577 \tValidation Loss: 345.738273\n",
            "[138,    10] loss: 0.039\n",
            "[138,    20] loss: 0.040\n",
            "[138,    30] loss: 0.038\n",
            "\tTraining Loss: 7.700876 \tValidation Loss: 337.466293\n",
            "[139,    10] loss: 0.043\n",
            "[139,    20] loss: 0.037\n",
            "[139,    30] loss: 0.041\n",
            "\tTraining Loss: 7.980457 \tValidation Loss: 340.904131\n",
            "[140,    10] loss: 0.040\n",
            "[140,    20] loss: 0.042\n",
            "[140,    30] loss: 0.036\n",
            "\tTraining Loss: 7.816536 \tValidation Loss: 332.625988\n",
            "[141,    10] loss: 0.043\n",
            "[141,    20] loss: 0.044\n",
            "[141,    30] loss: 0.051\n",
            "\tTraining Loss: 9.185437 \tValidation Loss: 428.334125\n",
            "[142,    10] loss: 0.047\n",
            "[142,    20] loss: 0.041\n",
            "[142,    30] loss: 0.044\n",
            "\tTraining Loss: 8.747474 \tValidation Loss: 325.170044\n",
            "[143,    10] loss: 0.043\n",
            "[143,    20] loss: 0.037\n",
            "[143,    30] loss: 0.047\n",
            "\tTraining Loss: 8.494290 \tValidation Loss: 340.185847\n",
            "[144,    10] loss: 0.042\n",
            "[144,    20] loss: 0.046\n",
            "[144,    30] loss: 0.043\n",
            "\tTraining Loss: 8.748517 \tValidation Loss: 347.151365\n",
            "[145,    10] loss: 0.044\n",
            "[145,    20] loss: 0.040\n",
            "[145,    30] loss: 0.039\n",
            "\tTraining Loss: 8.208962 \tValidation Loss: 343.949732\n",
            "[146,    10] loss: 0.037\n",
            "[146,    20] loss: 0.039\n",
            "[146,    30] loss: 0.043\n",
            "\tTraining Loss: 7.927188 \tValidation Loss: 430.684516\n",
            "[147,    10] loss: 0.043\n",
            "[147,    20] loss: 0.049\n",
            "[147,    30] loss: 0.036\n",
            "\tTraining Loss: 8.571904 \tValidation Loss: 342.678802\n",
            "[148,    10] loss: 0.034\n",
            "[148,    20] loss: 0.044\n",
            "[148,    30] loss: 0.040\n",
            "\tTraining Loss: 7.814579 \tValidation Loss: 337.261353\n",
            "[149,    10] loss: 0.038\n",
            "[149,    20] loss: 0.038\n",
            "[149,    30] loss: 0.036\n",
            "\tTraining Loss: 7.518810 \tValidation Loss: 336.002585\n",
            "[150,    10] loss: 0.043\n",
            "[150,    20] loss: 0.044\n",
            "[150,    30] loss: 0.035\n",
            "\tTraining Loss: 8.095943 \tValidation Loss: 349.030981\n",
            "[151,    10] loss: 0.045\n",
            "[151,    20] loss: 0.041\n",
            "[151,    30] loss: 0.040\n",
            "\tTraining Loss: 8.402288 \tValidation Loss: 345.126557\n",
            "[152,    10] loss: 0.047\n",
            "[152,    20] loss: 0.044\n",
            "[152,    30] loss: 0.041\n",
            "\tTraining Loss: 8.805802 \tValidation Loss: 335.141531\n",
            "[153,    10] loss: 0.036\n",
            "[153,    20] loss: 0.041\n",
            "[153,    30] loss: 0.039\n",
            "\tTraining Loss: 7.728386 \tValidation Loss: 347.963248\n",
            "[154,    10] loss: 0.042\n",
            "[154,    20] loss: 0.039\n",
            "[154,    30] loss: 0.037\n",
            "\tTraining Loss: 7.836962 \tValidation Loss: 417.185903\n",
            "[155,    10] loss: 0.037\n",
            "[155,    20] loss: 0.037\n",
            "[155,    30] loss: 0.042\n",
            "\tTraining Loss: 7.724143 \tValidation Loss: 428.534698\n",
            "[156,    10] loss: 0.041\n",
            "[156,    20] loss: 0.048\n",
            "[156,    30] loss: 0.042\n",
            "\tTraining Loss: 8.683252 \tValidation Loss: 357.497958\n",
            "[157,    10] loss: 0.041\n",
            "[157,    20] loss: 0.040\n",
            "[157,    30] loss: 0.035\n",
            "\tTraining Loss: 7.720820 \tValidation Loss: 335.994701\n",
            "[158,    10] loss: 0.041\n",
            "[158,    20] loss: 0.041\n",
            "[158,    30] loss: 0.036\n",
            "\tTraining Loss: 7.905824 \tValidation Loss: 328.959805\n",
            "[159,    10] loss: 0.039\n",
            "[159,    20] loss: 0.042\n",
            "[159,    30] loss: 0.034\n",
            "\tTraining Loss: 7.693862 \tValidation Loss: 341.828299\n",
            "[160,    10] loss: 0.036\n",
            "[160,    20] loss: 0.031\n",
            "[160,    30] loss: 0.034\n",
            "\tTraining Loss: 6.746331 \tValidation Loss: 344.654363\n",
            "[161,    10] loss: 0.036\n",
            "[161,    20] loss: 0.041\n",
            "[161,    30] loss: 0.036\n",
            "\tTraining Loss: 7.490839 \tValidation Loss: 341.458002\n",
            "[162,    10] loss: 0.037\n",
            "[162,    20] loss: 0.043\n",
            "[162,    30] loss: 0.041\n",
            "\tTraining Loss: 8.102876 \tValidation Loss: 334.470844\n",
            "[163,    10] loss: 0.048\n",
            "[163,    20] loss: 0.034\n",
            "[163,    30] loss: 0.037\n",
            "\tTraining Loss: 7.942635 \tValidation Loss: 343.348931\n",
            "[164,    10] loss: 0.040\n",
            "[164,    20] loss: 0.031\n",
            "[164,    30] loss: 0.038\n",
            "\tTraining Loss: 7.260434 \tValidation Loss: 350.706854\n",
            "[165,    10] loss: 0.036\n",
            "[165,    20] loss: 0.037\n",
            "[165,    30] loss: 0.032\n",
            "\tTraining Loss: 7.017105 \tValidation Loss: 350.102238\n",
            "[166,    10] loss: 0.038\n",
            "[166,    20] loss: 0.041\n",
            "[166,    30] loss: 0.035\n",
            "\tTraining Loss: 7.599829 \tValidation Loss: 352.273764\n",
            "[167,    10] loss: 0.037\n",
            "[167,    20] loss: 0.037\n",
            "[167,    30] loss: 0.038\n",
            "\tTraining Loss: 7.462825 \tValidation Loss: 346.864179\n",
            "[168,    10] loss: 0.036\n",
            "[168,    20] loss: 0.039\n",
            "[168,    30] loss: 0.035\n",
            "\tTraining Loss: 7.340316 \tValidation Loss: 350.730834\n",
            "[169,    10] loss: 0.036\n",
            "[169,    20] loss: 0.041\n",
            "[169,    30] loss: 0.034\n",
            "\tTraining Loss: 7.409402 \tValidation Loss: 342.516005\n",
            "[170,    10] loss: 0.034\n",
            "[170,    20] loss: 0.036\n",
            "[170,    30] loss: 0.042\n",
            "\tTraining Loss: 7.363600 \tValidation Loss: 342.400274\n",
            "[171,    10] loss: 0.040\n",
            "[171,    20] loss: 0.035\n",
            "[171,    30] loss: 0.040\n",
            "\tTraining Loss: 7.684716 \tValidation Loss: 349.761293\n",
            "[172,    10] loss: 0.040\n",
            "[172,    20] loss: 0.038\n",
            "[172,    30] loss: 0.035\n",
            "\tTraining Loss: 7.556776 \tValidation Loss: 354.129112\n",
            "[173,    10] loss: 0.039\n",
            "[173,    20] loss: 0.041\n",
            "[173,    30] loss: 0.036\n",
            "\tTraining Loss: 7.751684 \tValidation Loss: 344.699755\n",
            "[174,    10] loss: 0.044\n",
            "[174,    20] loss: 0.037\n",
            "[174,    30] loss: 0.042\n",
            "\tTraining Loss: 8.145772 \tValidation Loss: 344.616518\n",
            "[175,    10] loss: 0.036\n",
            "[175,    20] loss: 0.038\n",
            "[175,    30] loss: 0.038\n",
            "\tTraining Loss: 7.464351 \tValidation Loss: 342.136072\n",
            "[176,    10] loss: 0.039\n",
            "[176,    20] loss: 0.043\n",
            "[176,    30] loss: 0.039\n",
            "\tTraining Loss: 8.008712 \tValidation Loss: 346.355711\n",
            "[177,    10] loss: 0.037\n",
            "[177,    20] loss: 0.034\n",
            "[177,    30] loss: 0.037\n",
            "\tTraining Loss: 7.214474 \tValidation Loss: 460.286622\n",
            "[178,    10] loss: 0.049\n",
            "[178,    20] loss: 0.046\n",
            "[178,    30] loss: 0.038\n",
            "\tTraining Loss: 8.906945 \tValidation Loss: 335.743459\n",
            "[179,    10] loss: 0.036\n",
            "[179,    20] loss: 0.033\n",
            "[179,    30] loss: 0.040\n",
            "\tTraining Loss: 7.245244 \tValidation Loss: 479.215963\n",
            "[180,    10] loss: 0.038\n",
            "[180,    20] loss: 0.051\n",
            "[180,    30] loss: 0.040\n",
            "\tTraining Loss: 8.649595 \tValidation Loss: 349.081316\n",
            "[181,    10] loss: 0.039\n",
            "[181,    20] loss: 0.036\n",
            "[181,    30] loss: 0.035\n",
            "\tTraining Loss: 7.398385 \tValidation Loss: 342.199725\n",
            "[182,    10] loss: 0.039\n",
            "[182,    20] loss: 0.034\n",
            "[182,    30] loss: 0.037\n",
            "\tTraining Loss: 7.372206 \tValidation Loss: 443.730546\n",
            "[183,    10] loss: 0.037\n",
            "[183,    20] loss: 0.039\n",
            "[183,    30] loss: 0.040\n",
            "\tTraining Loss: 7.747604 \tValidation Loss: 349.887644\n",
            "[184,    10] loss: 0.035\n",
            "[184,    20] loss: 0.034\n",
            "[184,    30] loss: 0.036\n",
            "\tTraining Loss: 6.940666 \tValidation Loss: 353.331237\n",
            "[185,    10] loss: 0.029\n",
            "[185,    20] loss: 0.033\n",
            "[185,    30] loss: 0.036\n",
            "\tTraining Loss: 6.553704 \tValidation Loss: 346.962399\n",
            "[186,    10] loss: 0.037\n",
            "[186,    20] loss: 0.033\n",
            "[186,    30] loss: 0.039\n",
            "\tTraining Loss: 7.215566 \tValidation Loss: 331.635684\n",
            "[187,    10] loss: 0.031\n",
            "[187,    20] loss: 0.036\n",
            "[187,    30] loss: 0.031\n",
            "\tTraining Loss: 6.533220 \tValidation Loss: 333.203343\n",
            "[188,    10] loss: 0.035\n",
            "[188,    20] loss: 0.037\n",
            "[188,    30] loss: 0.036\n",
            "\tTraining Loss: 7.162857 \tValidation Loss: 340.774612\n",
            "[189,    10] loss: 0.037\n",
            "[189,    20] loss: 0.036\n",
            "[189,    30] loss: 0.041\n",
            "\tTraining Loss: 7.607134 \tValidation Loss: 338.180588\n",
            "[190,    10] loss: 0.033\n",
            "[190,    20] loss: 0.041\n",
            "[190,    30] loss: 0.035\n",
            "\tTraining Loss: 7.227582 \tValidation Loss: 347.914825\n",
            "[191,    10] loss: 0.034\n",
            "[191,    20] loss: 0.034\n",
            "[191,    30] loss: 0.036\n",
            "\tTraining Loss: 6.907348 \tValidation Loss: 354.116137\n",
            "[192,    10] loss: 0.035\n",
            "[192,    20] loss: 0.036\n",
            "[192,    30] loss: 0.036\n",
            "\tTraining Loss: 7.117439 \tValidation Loss: 340.786587\n",
            "[193,    10] loss: 0.038\n",
            "[193,    20] loss: 0.034\n",
            "[193,    30] loss: 0.033\n",
            "\tTraining Loss: 7.028862 \tValidation Loss: 331.695341\n",
            "[194,    10] loss: 0.037\n",
            "[194,    20] loss: 0.034\n",
            "[194,    30] loss: 0.033\n",
            "\tTraining Loss: 6.917612 \tValidation Loss: 332.679339\n",
            "[195,    10] loss: 0.033\n",
            "[195,    20] loss: 0.037\n",
            "[195,    30] loss: 0.031\n",
            "\tTraining Loss: 6.757824 \tValidation Loss: 331.855642\n",
            "[196,    10] loss: 0.037\n",
            "[196,    20] loss: 0.029\n",
            "[196,    30] loss: 0.033\n",
            "\tTraining Loss: 6.544472 \tValidation Loss: 331.330699\n",
            "[197,    10] loss: 0.035\n",
            "[197,    20] loss: 0.040\n",
            "[197,    30] loss: 0.033\n",
            "\tTraining Loss: 7.189513 \tValidation Loss: 345.755152\n",
            "[198,    10] loss: 0.033\n",
            "[198,    20] loss: 0.040\n",
            "[198,    30] loss: 0.029\n",
            "\tTraining Loss: 6.836797 \tValidation Loss: 343.622824\n",
            "[199,    10] loss: 0.032\n",
            "[199,    20] loss: 0.035\n",
            "[199,    30] loss: 0.031\n",
            "\tTraining Loss: 6.498424 \tValidation Loss: 337.045151\n",
            "[200,    10] loss: 0.031\n",
            "[200,    20] loss: 0.033\n",
            "[200,    30] loss: 0.030\n",
            "\tTraining Loss: 6.249570 \tValidation Loss: 340.050519\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "## Training\n",
        "checkpoint = \"./checkpoint/save.pt\"\n",
        "if not os.path.isfile(checkpoint):\n",
        "  os.makedirs(\"./checkpoint\")\n",
        "  #改\n",
        "  trainer(net, criterion, optimizer, trainloader, testloader, epoch_n=200, path=checkpoint)\n",
        "else:\n",
        "  net.load_state_dict(torch.load(checkpoint))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7t_gO-3AY2i"
      },
      "source": [
        "# Test the Data\n",
        "Now let's compare with the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t_DRUA3l_tdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac83d8f-727e-444c-d70f-a7b59933d2ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Result:  2010.1990966796875\n"
          ]
        }
      ],
      "source": [
        "test = tester(net, criterion, testloader)\n",
        "# Show the difference between predict and groundtruth (loss)\n",
        "print('Test Result: ', test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKN0jGueAnhN"
      },
      "source": [
        "# Predict\n",
        "Our model is now ready! \n",
        "\n",
        "Let's try with some sample data!\n",
        "Suppose the closing data, \n",
        "* Monday = 126\n",
        "* Tuesday = 124\n",
        "* Wednesday = 124\n",
        "* Thursday = 122.5\n",
        "* Friday = 121\n",
        "\n",
        "So, the model is predicting the next day given the input of 5 day closing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "70ehxcuAAiZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1339b8fc-958d-4cb2-e4f4-a5e709094b86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Result 114.95565795898438\n"
          ]
        }
      ],
      "source": [
        "predict = net.predict(torch.tensor([[126, 124, 124, 122.5, 121]], dtype=torch.float32).to(device))\n",
        "print('Predicted Result', predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36tyoplUCIov"
      },
      "source": [
        "What can you do to improve the performance?\n",
        "\n",
        "* Increase the epoch\n",
        "* You can use multiple input data (Maybe closing data and volume has a relation!)\n",
        "* Try out different loss function\n",
        "* Evaluate the test data and training data and see the accuracy \n",
        "* Experiment with the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YlKIBPMGmYO"
      },
      "source": [
        "# Report Guidelines\n",
        "In the field of Computer Science, most of the latest techniques are proposed at conferences instead of journal, like NIPS, ACL, CVPR, AAAI and so on.\n",
        "\n",
        "We will follow IEEE guidelines to write paper report.\n",
        "A proper report will contain-\n",
        " * Abstract\n",
        " * Introduction\n",
        " * Methods\n",
        " * Experiment\n",
        " * Results\n",
        " * Conclusion \n",
        " * References\n",
        " For more details: https://www.ieee.org/conferences/publishing/templates.html\n",
        " We recommend to use overleaf latex Editor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTSKbgXQgVAS"
      },
      "source": [
        "# Grading\n",
        " Grading is based on your report.\n",
        "Below are the grading policy for this homework:\n",
        "* The report should be in English.\n",
        "* Introduction & Conclusion (30%)\n",
        "* Methods (30%)\n",
        "* Experiment & Results (30%)\n",
        "* Abstract & References (10%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtH-h71dHgzS"
      },
      "source": [
        "# Final Project\n",
        "We suggest selecting one interesting topic as early as possible. Most of the students here are undergraduate or new to coding, it would be the best time for you to develop your knowledge and the ability to implement code by reading research papers. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWRSXmOy1aLM"
      },
      "source": [
        "# Papers\n",
        "You can find papers here:\n",
        "* https://arxiv.org\n",
        "* Google Scholar\n",
        "* Conference Websites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WVvzCvI61z81"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
